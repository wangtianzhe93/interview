# LoRA

## Concepts

- 1 **LoRA 的核心思想**
    - LoRA (Low Rank Adaptation) 本质上是对特征矩阵进行低秩分解的一种近似数值分解技术, 可以大幅降低特征矩阵的参数量, 但是会伴随一定的有损压缩. 本质上是用更少的参数来近似大模型全参数微调所得的增量参数, 从而使用更小的显存完成高效的微调.
    - 语言建模的基本符号定义是, 最大化给定提示的条件概率 (极大似然估计)
        - 语言模型的条件概率分布建模目标, 调整模型参数$\Phi$, 使得模型在预测每一个训练样本的输出序列时, 每一步都能以尽可能高的概率预测出正确的词或标记.
            $$\max_{\Phi} \sum_{(x,y) \in Z} \sum_{t=1}^{|y|} \log(P_{\Phi}(y_t | x, y_{<t}))$$
            - $\sum_{(x,y) \in Z}$: 表示对数据集 Z 中的所有样本对 (x,y) 进行求和。
            - $\sum_{t=1}^{|y|}$: 表示对输出序列 y 中的每一个时间步 t（从1到序列长度 ∣y∣）进行求和。
            - $\log(P_{\Phi}(y_t | x, y_{<t}))$: 表示在给定输入 x 和先前已生成的序列部分 $y < t$ 的条件下, 模型$\Phi$预测当前时间步$t$的输出为 $y_t$ 的概率的对数。
        - 给定一个参数为$\Phi$的预训练自回归语言模型$p_{\Phi}(y|x)$, $x$是输入, $y$是输出.
        - 在全参数微调(full fine tuning)里, 模型会学习到一个$\Delta \Phi$, $\Delta \Phi$是特定与下游任务的增量参数, 参数量会非常大(与现有参数量一样), 训练成本高.
        - 在LoRA为代表的高效微调中, 令 $\Delta \Phi = \Delta \Phi (\Theta)$, 用参数量更少的$\Theta$来编码, 即用低秩降维表示来近似, $\Theta << \Phi$, 上面的公式可以改写为:
            $$\max_{\Theta} \sum_{(x,y) \in Z} \sum_{t=1}^{|y|} \log(P_{\Phi_0 + \Delta \Phi (\Theta)}(y_t | x, y_{<t}))$$

    - 明确 LoRA 涉及的参数
        - Transformer 层的输入和输出维度大小是$d_model$.
        - $W_q$ $W_k$ $W_v$ 和 $W_o$ 分别代表了自注意力的 query, key, value 和 output 的 projection 矩阵.
        - $W$ 或 $W_0$ 代表了预训练的权重矩阵.
        - $\Delta W$ 代表了微调后得到的增量参数矩阵, 即训练后优化算法在参数上的累积更新量.
        - $r$ 代表了 LoRA 的地址适应参数的秩.

    - LoRA 核心思想
        - 在冻结预训练模型权重之后, 将可训练的低秩分解矩阵注入到 Transformer 架构的每一层中, 大大减少在下游任务上的可训练参数量.
        - 在推理时, 可以直接把原来预训练模型权重与训练好的 LoRA 权重合并, 因此在推理时不存在额外的开销.
        ![LoRA结构](LoRA/2.png)

    - LoRA 产生的背景
        - 通常, 冻结预训练权重, 再额外插入可训练的权重是常规做法 (Adapter), 问题在于, 这样做不仅额外增加了参数, 同时改变了模型结构, 因为它们往往把新的参数加入到模型内部, 会导致模型训练 推理的计算成本和内存占用急剧增加, 尤其是在模型参数需要在多GPU上分布式推理时.
        - 深度网络由大量的 dense 层组成, 这些参数矩阵通常是满秩的.
        - 在特定任务上, 训练得到的过度参数化的模型实际上存在于一个较低的内在维度上(高维数据实际存在于低维子空间中). LoRA 假设, 大模型在下游任务上微调得到的增量参数矩阵 $\Delta W$ 是低秩的(至少不是满秩), 肯定存在冗余参数或高度相关的参数矩阵, 其中实际有效参数是更低维度的. 因此设想, 可以对全参数微调的增量参数矩阵 $\Delta W$ 进行低秩分解近似表示, 对其参数做降维.
        - 这样训练 $\Delta W$ 的低秩分解近似参数矩阵, 效果上相比其他的 PEFT 方法相近, 而且还能在推理时不增加额外的开销.

- 2 **LoRA 原理叙述**
    - LoRA 做的其实是低秩矩阵自适应, 在冻结原有的大模型参数时, 用参数量更小的矩阵进行低秩近似训练.
    - 对于预训练权重矩阵 $W_0 \in R^{d \times d}$, LoRA 限制了其更新的方式, 也就是把全参数微调的增量参数矩阵 $\Delta W$ 表示为两个参数量更小的矩阵 $A$ 和 $B$ 的低秩近似:
        $W_0 + \Delta W = W_0 + B \times A$
        - 其中, $B \in R^{d \times r}$ 和 $A \in R^{r \times d}$ 是 LoRA 的低秩适应权重矩阵, 秩 $r << d$.
        - 此时, 微调的参数量从原来的 $\Delta W$ 的 $d \times d$, 变成了 $B$ 和 $A$ 的 $2 \times r \times d$, 远小于原来的参数量.
        - 给定输入 $x \in R^d$, 添加 LoRA 之后的输出 $h \in R^d$:
            $\Delta h = (W_0 + \Delta W)x = W_0 X + B \times A x$
            因为 $h$ 只与 $B \times A$ 相关, 因此可以认为 $ h = B \times A x$.
        - 在训练时, 原始参数 $W_0$ 被冻结, 意味着 $W_0$ 虽然会参与前向传播和反向传播, 但是不会计算其对应梯度 $\frac{\partial L}{\partial W_0}$, 也不会更新这部分的参数.
        - 在推理时, 直接按照上面的公式将 $B \times A$ 合并到 $W_0$ 中, 因此相比原始的大模型理论上不存在推理延时.

    - 假设大模型的原始权重矩阵 ($W$):
    <div style="margin-left: 2em;">

    $$  
        \left[
        \begin{matrix}
        1.0 & 2.0 & 3.0 & 4.0 \\
        5.0 & 6.0 & 7.0 & 8.0 \\
        9.0 & 10.0 & 11.0 & 12.0 \\
        13.0 & 14.0 & 15.0 & 16.0 \\
        17.0 & 18.0 & 19.0 & 20.0 \\
        \end{matrix}
        \right]
    $$

    </div>

    - 全量微调需要更新全部的参数, $M \times N$, 即 $5 \times 4 = 20$.
        - 假设全量微调之后的参数是

            $$  
                W^{\prime} = 
                \left[
                \begin{matrix}
                1.41 & 2.44 & 3.47 & 4.50 \\
                5.93 & 7.00 & 8.07 & 9.14 \\
                10.45 & 11.56 & 12.67 & 13.78 \\
                14.97 & 16.12 & 17.27 & 18.42 \\
                19.49 & 20.68 & 21.87 & 23.06 \\
                \end{matrix}
                \right]
            $$

        - 这个微调之后的参数可以表达为原来的矩阵加上增量矩阵

            $$  
                W^{\prime} = W + \Delta W = 
                \left[
                \begin{matrix}
                1.0 & 2.0 & 3.0 & 4.0 \\
                5.0 & 6.0 & 7.0 & 8.0 \\
                9.0 & 10.0 & 11.0 & 12.0 \\
                13.0 & 14.0 & 15.0 & 16.0 \\
                17.0 & 18.0 & 19.0 & 20.0 \\
                \end{matrix}
                \right]
                + 
                \left[
                \begin{matrix}
                0.41 & 0.44 & 0.47 & 0.50 \\
                0.93 & 1.00 & 1.07 & 1.14 \\
                1.45 & 1.56 & 1.67 & 1.78 \\
                1.97 & 2.12 & 2.27 & 2.42 \\
                2.49 & 2.68 & 2.87 & 3.06 \\
                \end{matrix}
                \right]
            $$

    - 进一步, 可以把 $\Delta W$ 分解为两个矩阵的乘积, 这个增量矩阵就可以使用 LoRA 低秩分解, 即 $\Delta W = A \times B$.

        $$
            \Delta W = A \times B = 
            \left[
            \begin{matrix}
            0.1 & 0.2 \\
            0.3 & 0.4 \\
            0.5 & 0.6 \\
            0.7 & 0.8 \\
            0.9 & 1.0
            \end{matrix} 
            \right]
            \times
            \left[
            \begin{matrix}
            1.1 & 1.2 & 1.3 & 1.4 \\
            1.5 & 1.6 & 1.7 & 1.8
            \end{matrix} 
            \right]
        $$

        - 因此, 最初的目标是训练 $W^{\prime}$, 变成了训练 $\Delta W$, LoRA 是训练 $A \times B$ 来近似得到结果.

    - 参数下降量
        - $A$ 有 $5 \times 2 = 10$ 个参数.
        - $B$ 有 $2 \times 4 = 8$ 个参数.
        - LoRA 总参数 $10 + 8 = 18$ 个参数.
        - 通过 LoRA 微调, 调参对象从 $\Delta W$ 变为 $A$ 和 $B$, 从 20 减少到 18, 这只是简化的例子. 在实际案例中, 参数量可以减少为原来的 0.01%-3% 左右.

- 3 **低秩分解的原理**
    - 低秩分解的核心思想: 矩阵的里面的信息往往不是均匀分布的, 很多维度都是冗余的, 只需要抓住主要的方向就可以.

    - 矩阵的秩 (Rank)
        - 在线性代数中, 一个矩阵的秩 (rank) 是它的线性独立行或列的数量. 如果一个矩阵是"低秩"的, 意味着它的信息可以用少量独立方向分量表达, 而不需要完整的维度.
        - 比如下面矩阵, 第5行 $([1,2,0,3,0])$ 是第1行 $([1,0,0,2,0])$ 和第2行 $([0,2,0,1,0])$ 的线性组合 (第5行=第1行+第2行), 第5行没有提供更多的信息, 理论上这个矩阵有前4行就能提供所有信息了, 因此矩阵的行秩为4(列秩也为4, 第5列全为 0, 没有信息增量).

        $$
            S = 
            \left[
            \begin{matrix}
            1 & 0 & 0 & 2 & 0 \\
            0 & 2 & 0 & 1 & 0 \\
            0 & 0 & 3 & 0 & 0 \\
            2 & 1 & 0 & 5 & 0 \\
            1 & 2 & 0 & 3 & 0 \\
            \end{matrix}
            \right]
        $$

    - 奇异值分解, 低秩分解
        - 奇异值分解 (Singular Value Decomposition), 可以把任意一个矩阵分解成三个矩阵的乘积, 对于一个形状是 $d \times k$ 的矩阵 $W$, 奇异值分解可以写成 $S = U \times \Sigma \times V^T$, 其中
            - $U$ 是 $d \times d$ 的正交矩阵, 是 $d \times k$ 的对角矩阵(奇异值), $V^T$ 是 $k \times k$ 的正交矩阵.
            - $r$ 是矩阵的秩, 决定了分解后保留的信息量. 如果只保留最大的几个奇异值(低秩近似), 就能用更少的参数近似 $W$.
        - 假设有这个矩阵

            $$
                S = 
                \left[
                \begin{matrix}
                1 & 0 & 0 & 2 & 0 \\
                0 & 2 & 0 & 1 & 0 \\
                0 & 0 & 3 & 0 & 0 \\
                2 & 1 & 0 & 5 & 0 \\
                1 & 2 & 0 & 3 & 0 \\
                \end{matrix}
                \right]
            $$

        - 可以分解为以下三个矩阵:

            $$
                U \approx 
                \left[
                \begin{matrix}
                0.3 & 0 & 0.34 & -0.68 & -0.58 \\
                -0.22 & 0 & -0.76 & 0.2 & -0.58 \\
                0 & -1 & 0 & 0 & 0 \\
                -0.77 & 0 & 0.36 & 0.52 & 0 \\
                -0.52 & 0 & -0.42 & -0.48 & -0.58 \\
                \end{matrix}
                \right]
                \Sigma = 
                \left[
                \begin{matrix}
                7.03 & 0 & 0 & 0 & 0 \\
                0 & 3 & 0 & 0 & 0 \\
                0 & 0 & 2.15 & 0 & 0 \\
                0 & 0 & 0 & 0.11 & 0 \\
                0 & 0 & 0 & 0 & 0 \\
                \end{matrix}
                \right]
                V \approx
                \left[
                \begin{matrix}
                0.34 & -0.32 & 0 & -0.89 & 0 \\
                0 & 0 & -1 & 0 & 0 \\
                0.3 & -0.93 & 0 & 0.22 & 0 \\
                -0.89 & -0.19 & 0 & 0.41 & 0 \\
                0 & 0 & 0 & 0 & 1 \\
                \end{matrix}
                \right]
            $$

            - 其中, 这个矩阵奇异值 $\Sigma$ 中大于0的数量就是矩阵的秩.
        - 如果只保留最大的三个奇异值, 对原矩阵进行重构, 保留 $\sigma_1 = 7.03$, $\sigma_2 = 3$, $\sigma_3 = 2.15$, 重构矩阵过程如下:

            $$
                U \approx 
                \left[
                \begin{matrix}
                0.3 & 0 & 0.34 \\
                -0.22 & 0 & -0.76 \\
                0 & -1 & 0 \\
                -0.77 & 0 & 0.36 \\
                -0.52 & 0 & -0.42 \\
                \end{matrix}
                \right]
                \Sigma_{trunc} = 
                \left[
                \begin{matrix}
                7.03 & 0 & 0 \\
                0 & 3 & 0 \\
                0 & 0 & 2.15 \\
                \end{matrix}
                \right]
                V \approx
                \left[
                \begin{matrix}
                0.34 & -0.32 & 0 & -0.89 & 0 \\
                0 & 0 & -1 & 0 & 0 \\
                0.3 & -0.93 & 0 & 0.22 & 0 \\
                \end{matrix}
                \right]
            $$

        - 把这三个矩阵乘回去, $S^\prime = U_{trunc} \times \Sigma_{trunc} \times V^T_{trunc}$, 计算得到重构后的结果 $S^\prime$:

            $$
                S^\prime = 
                \left[
                \begin{matrix}
                0.93 & -0.01 & 0 & 2.03 & 0 \\
                0.02 & 2 & 0 & 0.99 & 0 \\
                0 & 0 & 3 & 0 & 0 \\
                2.05 & 1.01 & 0 & 4.98 & 0 \\
                0.95 & 1.99 & 0 & 3.02 & 0 \\
                \end{matrix}
                \right]
            $$

        - 对比原始矩阵和重构矩阵, 结果基本一致. 结论: 如果只保留最大的几个奇异值(低秩近似), 就能够用更少的参数近似全参数矩阵 $W$.
        - 同时, 还可以通过保留下来的奇异值, 计算重构后的矩阵保留了多少信息, 把保留下来奇异值的平方和, 除以原来中奇异值的平方和即可.
        - 类似上面的矩阵, 信息量主要集中在少数几个方向的矩阵被称为低秩矩阵, 低秩矩阵可以通过奇异值分解, 降低参数量.

    - $\Delta W$ 的低秩分解
        - 通过对微调后的权重变化么的奇异值分解, 大部分信息集中在少数几个奇异值上. 在 LoRA 论文 ([Huet al., 2021](https://arxiv.org/pdf/2106.09685)) 在 GPT-3 上测试时发现, $\Delta W$ 的前 10-20 个奇异值就占据了 90% 以上的信息.
        - 这意味着, 当我们从一个预训练模型 (比如 Llama 或 BERT) 微调到某个特定任务时, 权重矩阵的变化(即 $\Delta W = W_{任务} - W_{预训练}$) 并不是完全随机的, 而是具有某种"结构化"特性. 这个变化可以用一个低秩矩阵来近似表示, 它的有效自由度比原始权重矩阵的维度要少得多. 由于 $\Delta W$ 的秩很低, 可以用 $A$ 和 $B$ 直接构造, 不需要完整的 SVD 计算, 更加高效.
        - 直观理解, 微调只是为了强化模型在某个特定领域的能力, 不需要对所有方向的参数进行调整. 想象预训练模型是一个已经学会"说话"的智能体, 微调只是让它学会某种特定的"口音" (比如工业领域的术语和固定说法). 这个"口音"调整不需要重新学习所有语言规则, 只需要在少数关键方面 (例如词汇选择, 用法等) 做改变即可, 这种"自由度"很低的改变, 可以用低秩矩阵进行近似.
        - Example
            - 一个 $512 \times 512$ 的权重矩阵 ($W$)(总共 262,144 个参数):
                - 全微调: 可能调整所有 262,144个参数.
                - LORA: 假设 $r = 8$, 只用 $A$ ($512 \times 8$) 和 $B$ ($8 \times 512$), 总共8,192 个参数, 就能捕捉任务的主要变化. 因为 $\Delta W$ 的变化集中在少数方向 (比如8个, 需要调参), 而不是需要 512 个方向.

    - 原始矩阵 $W$
        - 一般不做低秩分解, 预训练模型 (BERT) 的权重矩阵 ($W$), 通常具有较高的秩 (满秩), 奇异值分布比较平滑.    

## Train

- 1 **LoRA更新参数**
    - 本质上是反向传播算法, 在前向过程, 代入 $A$ 和 $B$ 的参数计算出损失, 在反向过程, 根据损失求导算出 $A$ $B$ 参数的梯度, 然后更新参数.
    - $$W^\prime = W + \Delta W = W + A \times B$$
        - $W^\prime$ 是调整之后的有效权重, 用于前向传播, $W$ 是预训练模型的原始权重矩阵, 形状为 $(d \times k)$, $\Delta W$ 是 LoRA 引入的调整量 (增量), 形状为 $(d \times k)$.
        - $A$ 是形状为 $(d \times r)$ 的矩阵, r是矩阵的秩, 远远小于 d 和 k , $B$ 形状也是 $(d \times k)$, $A \times B$ 结果是 $(d \times k)$, 和 $W$ 形状一致.

    - 初始化
        - $W$ 是预训练好的权重, 固定不动 (即 $$W$ 的梯度设为0), $A$ 和 $B$ 是 LoRA 的可训练参数.
        - $A$ 通常用小的随机值初始化: $A \sim \mathcal N(0, \sigma^2)$，$\alpha$ 很小.
        - $B$ 初始可以设为全 0 (论文中常用), 这样初始时 $\Delta W = 0$, 不会干扰原始模型.

    - 计算损失
        - 对于输入 $X$ 和目标输出 $Y_{true}$, 前向传播计算预测输出: $Y_{pred} = (W + A \times B) \times X$.
        - 预测值和真实值进行比较, 得到损失函数. 根据任务定义损失函数 $L$ (比如交叉熵损失). $L = Loss(Y_{pred}, Y_{true})$.

    - 反向传播更新
        - 计算损失 $L$ 对A和B的梯度, $\frac{\partial L}{\partial A}$, $\frac{\partial L}{\partial B}$.
        - 使用梯度下降 (或其他优化器如 Adam) 更新 $A$ 和 $B$:
            $$A \leftarrow A - \eta \cdot \frac{\partial L}{\partial A}$$
            $$A \leftarrow B - \eta \cdot \frac{\partial L}{\partial B}$$
            其中 $\eta$ 是学习率.

    - 迭代优化
        - 重复步骤2和3, 直到损失收敛或达到预定训练轮次.
        - 最终, $A$ 和 $B$ 被训练成能适配新任务的参数, 而保持不变.
        - 训练完成后, 可以通过 $W^\prime = W + \alpha \cdot \frac{A \times B}{r}$ 使用合并后的权重进行推理, 也可以保持分离状态以便切换不同任务.

- 2 **LoRA 训练时的参数变化**
    - 初始化阶段
        - 在开始训练时, 矩阵 $B$ 通过高斯函数初始化, 分布为 $b_i ~ N(0, \sigma_b^2)$. 矩阵 $A$ 为全零初始化, $a_{i,j} = 0$.
        - 在使得训练开始前, LoRA 的旁路 $B \times A = 0$, 那么微调就能够从预训练权重 $W_0$ 开始, 在训练时可以跟全参数微调一样, 有相同的开始. 这个策略要求 $B$ 和 $A$ 中至少有一个被初始化成全零, 如果 $B$ 和 $A$ 都被初始化成全零, 全部的梯度也是0, 因此设置 $A$ 为全零.

    - 合并参数阶段
        - 在实现中, $\Delta W = B \times A$ 会乘以系数 $\frac{\alpha}{r}$ 与原始预测值权重 $W_0$ 合并:
            $$h = \left(W_0 + \frac{\alpha}{r} \Delta W\right)x$$

        - 系数 $\frac{\alpha}{r}$ 决定了在下游任务上微调得到的 LoRA 低秩适应的权重矩阵 $B \times A$ 占最终模型参数的比例.
        - 给定一个或多个下游任务数据, 进行 LoRA 微调:
            - 系数 $\frac{\alpha}{r}$ 越大, LoRA 微调权重的影响就越大, 在下游任务上越容易过拟合  
            - 系数 $\frac{\alpha}{r}$ 越小, LoRA 微调权重的影响就越小(微调的效果不明显, 原始模型参数受到的影响也较少).

        - 一般来说, 在给定任务上 LoRA 微调, 一般设置参数 $\alpha$ 为 $r$ 的 2 倍数. 根据经验, LoRA 训练大概很难进入新的知识, 更多是修改 LLM 的指令等随的能力, 例如输出风格和格式. 原始的 LLM 能力, 是在预测结果获得的(取决于参数量 数据规模 数据质量).
        - LoRA 的秩 $r$ 决定 LoRA 的低秩近似矩阵的拟合能力, 实际任务需要调整挑选合适的秩 $r$ 维度. 系数 $\frac{\alpha}{r}$ 中 $\alpha$ 决定新老权重的占比.

- 3 **LoRA 用在 Transformer 哪些层**
    - 并不是所有参数都要微调, 选择部分参数进行微调就可以取得比较好的效果.
    ![Transformer](LoRA/4.png)
    - LoRA 的集成主要是将低秩增量 $\Delta W = A \times B$ 添加到这些层的权重矩阵上, 同时保持原始权重冻结.
    - 注意力层
        - Transformer 的核心是多头注意力机制, 每个头有Query Key Value Output, 即 $W_q, W_k, W_v, W_o$ 四个权重矩阵.
        - LoRA 通常应用在 $W_q$ 和 $W_v$ 上, $W_q$ 决定关注哪些信息, $W_v$ 决定输出什么内容. 调整它们能直接影响模型对任务的理解和生成能力. LoRA 原论文发现微调 $W_q$和 $W_v$ 就能接近全参数微调效果, 性价比高.
        - 对 $W_q$ 引入 $A_q$ 和 $B_q$, 使 $W^{\prime}_q = W_q + A_q \times B_q$.
        - 对 $W_v$ 引入 $A_v$ 和 $B_v$, 使 $W^{\prime}_v = W_q + A_q \times B_q$.

        - $W_k$ 和 $W_o$ 有时也会加 LoRA, 一般很少使用, 视任务需求而定. $W_k$ 与 $W_o$功能重叠, 单独调整收益有限. $W_o$是多头合并后的投影, 调整不如 $W_v$ 直接.
    
    - 前馈层(FFN)
        - FFN 包含两个线性变换: $W_1$ (升维) 和 $W_2$ (降维)
        - 对 $W_1$, 引入 $A_1$ 和 $B_1$, 使 $W^{\prime}_1 = W_1 + A_1 \times B_1$.
        - FFN 处理注意力输出, 负责特征提取和非线性映射, 调整它能增强任务特定表达.
        - 小模型(如 BERT)可能只加注意力层, 大模型(如GPT-3)或生成任务常扩展到 FFN.

    - 其他层
        - 嵌入层 (Embedding) LayerNorm 和偏置 (Bias) 通常保持冻结, 不用调整.
        - 嵌入层权重大但变动小, 冻结节省资源.
        - LayerNorm 参数少, 直接更新成本低.
        - 偏置在梯度更新中学习速度快, 不需要 LoRA 辅助.

- 3 **选择 LoRA 的秩**
    - LoRA 的低秩分解近似矩阵 B 和 A 的秩 r 的大小, 决定了 LoRA 的拟合能力. 其本质与推荐系统中的评分矩阵分解, 文本的非负矩阵分解, 奇异值分解一样.
    - 理想的情况是找到一个合适的秩 r, 使得 LoRA 的低秩近似结构 $B \times A$ 能具备全参数微调的增量矩阵 $\Delta W$ 的表达能力, 能力越接近越好.
    - 秩 r 成为了 LoRA 的超参数, 随着秩的维度不断增加, 参与训练的数据量也随之增加, LoRA 的低秩适应能力将逐渐提高直到过拟合.
    ![Select Rank](LoRA/3.png)
    - Weight Type 指的是对 Attention 的部分参数做了低秩适应. 可以发现, 在两个数据集上, r = 4, 8 时基本上略优于 r = 64 的效果, 更高的 r 不一定带来更好的效果.
    - 选取 r 的经验
        - 下游任务: 简单任务需要的秩不大, 任务难度越高 多任务混合等情况, 需要更大的秩.
        - 基座能力: 越强的基座, 所需要的秩更小. 例如 Qwen2-72B-Instruct 对比 Qwen2-7B-Instruct, 需要的秩肯定更小.
        - 数据规模: 数据规模越大, 需要更大的秩.

- 4 **LoRA 的改进**
    - LoRA+
        - 基础: 基于标准 LoRA, LoRA+ 的核心改进是对 $A$ 和 $B$ 矩阵设置不同的学习率, 加速收敛.
        - 实现: 通常 $B$ (靠近输出)的学习率 $\eta B$ 远高于 $A$ (靠近输入) 的学习率 $\eta A$, 比例 $\lambda = \frac{\eta B}{\eta A}$ 范围在 4-16.
        - 理论依据: 神经网络中靠近输出的权重对梯度变化更敏感, 需要更大的调整, 而靠近输入的权重应更稳定.
        - 优点: 训练速度提升(最高可达2倍), 性能比标准 LoRA 高 1%-3%.

    - DoRA

    - rs LoRA

    - PiSSA

    - GaLore

- 5 **LoRA 调整超参数**
    | 参数名 | 类型 | 含义  | 建议 | 默认值 |
    |-----|-----|-----|-----|-----|
    | finetuning_type | Literal["full", "freeze", "lora"] | 指定微调类型为 LoRA | 选择 `lora`，用于高效微调 | `lora`   |
    | lora_alpha | Optional[int] | 缩放系数 \(\alpha\)，一般情况下为 `lora_rank * 2` | 设为 `r` 的 1-2 倍，如 \(r = 16, \alpha = 32\) | `None` |
    | lora_dropout | float (0.0-1.0) | LoRA 的 Dropout 概率 | 大数据集：`0.0`；小数据集：`0.05-0.1` | `0.0` |
    | lora_rank | int | LoRA 的秩 \(r\) | 简单任务：`8-16`；中等：`32`；复杂：`64+` | `8` |
    | lora_target | str（逗号分隔）| 应用 LoRA 方法的模块名称 | 默认：`q_proj, v_proj`；复杂任务加 `k_proj, o_proj` 或 FFN | `all` |
    | additional_target | str（逗号分隔）| 额外的 LoRA 目标模块 | 通常留空，除非有特殊模块 | `None` |
    | loraplus_lr_ratio | Optional[float] | LoRA+ 学习率比例 (\(\lambda = \frac{\eta_B}{\eta_A}\)) | 启用时设 `4-16`（如 `8`）| `None` |
    | use_rslora | bool | 是否启用秩稳定 LoRA (rsLoRA) | 小 \(r\)（4-16）：`false` | `false`  |
    | use_dora | bool | 是否启用权重分解 DoRA | 默认 `false`，复杂任务开启 | `false`  |
    | create_pissa_init | bool | 是否用 PI+SSA 初始化 LoRA | 默认 `false`，需高性能时设 `true`（禁用量化模型）| `false` |

    - 从小秩开始: 对于大多数任务, 可以从 $r = 8$ 或 $r = 16$ 开始调整, 评估性能后再决定是否需要更高的秩.
    - 数据集大小与秩的关系: 小数据集(<5K样本) 用小秩 ($r=8$); 大数据集 (>50K样本) 可以尝试更大秩 ($r = 32+$).
    - 复杂任务策略: 对于复杂推理任务, 可以结合使用: (1) 增大 $r$ 到32或64; (2) 启用 rSLORA; (3) 添加更多目标层(如 FFN).



    
